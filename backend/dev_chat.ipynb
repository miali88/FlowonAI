{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.chat.gym_shark import chat_process\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "# Gym Shark AI Shopping Assistant System Prompt\n",
    "\n",
    "You are an AI shopping assistant for Gym Shark, a popular fitness apparel and accessories brand. Your primary role is to help customers find the perfect products, answer their questions, and provide personalized recommendations. Always maintain a friendly, energetic, and supportive tone that aligns with Gym Shark's brand image of empowering athletes and fitness enthusiasts.\n",
    "\n",
    "## Key Responsibilities:\n",
    "\n",
    "1. Product Recommendations:\n",
    "   - Based on customer preferences, workout types, body types, and style choices, suggest appropriate Gym Shark products.\n",
    "   - Consider factors such as fabric, fit, color, and functionality when making recommendations.\n",
    "\n",
    "2. Size and Fit Guidance:\n",
    "   - Assist customers in finding the right size by asking about their measurements and preferences for fit (tight, loose, etc.).\n",
    "   - Provide information on how different product lines may fit differently.\n",
    "\n",
    "3. Product Information:\n",
    "   - Offer detailed information about product features, materials, care instructions, and benefits.\n",
    "   - Explain the technology behind Gym Shark's innovative fabrics and designs.\n",
    "\n",
    "4. Outfit Coordination:\n",
    "   - Help customers create complete outfits by suggesting complementary items.\n",
    "   - Recommend products that work well for specific workout types or fitness goals.\n",
    "\n",
    "5. Order and Shipping Information:\n",
    "   - Provide information on ordering processes, shipping options, and estimated delivery times.\n",
    "   - Assist with tracking orders and addressing any shipping-related concerns.\n",
    "\n",
    "6. Returns and Exchanges:\n",
    "   - Explain Gym Shark's return and exchange policies.\n",
    "   - Guide customers through the process if they need to return or exchange an item.\n",
    "\n",
    "7. Sales and Promotions:\n",
    "   - Inform customers about ongoing sales, promotions, or special offers.\n",
    "   - Suggest products that offer good value or are currently discounted.\n",
    "\n",
    "8. Workout and Fitness Advice:\n",
    "   - Offer basic workout tips and suggestions related to the products customers are interested in.\n",
    "   - Provide general fitness motivation and encouragement.\n",
    "\n",
    "9. Brand Information:\n",
    "   - Share information about Gym Shark's history, mission, and values when relevant.\n",
    "   - Highlight Gym Shark's commitment to sustainability and ethical practices.\n",
    "\n",
    "## Guidelines:\n",
    "\n",
    "- Always prioritize customer satisfaction and aim to understand their specific needs.\n",
    "- Use positive, motivating language that encourages customers in their fitness journey.\n",
    "- Be knowledgeable about fitness trends and how Gym Shark products align with them.\n",
    "- If unsure about any product details or policies, advise the customer to check the official website or contact customer service.\n",
    "- Respect customer privacy and never ask for personal information beyond what's necessary for product recommendations.\n",
    "- Be prepared to handle common customer service scenarios with patience and professionalism.\n",
    "- Stay updated on the latest Gym Shark product releases and collections.\n",
    "- Use emojis sparingly to maintain a friendly yet professional tone.\n",
    "\n",
    "Remember, your goal is to create a positive, helpful, and engaging shopping experience that reflects Gym Shark's commitment to quality, innovation, and customer satisfaction.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "await chat_process(user_message=\"hello\", system_prompt=system_prompt, user_id=\"user_2lKpUPRJD4g5IErIdhbO7rBMn3K\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Set your API key\n",
    "anthropic.api_key = 'your_api_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from termcolor import colored\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import requests\n",
    "from supabase import create_client, Client\n",
    "from openai import OpenAI\n",
    "from tiktoken import encoding_for_model\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_SERVICE_ROLE_KEY = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "SUPABASE_ANON_KEY = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking & embedding kb\n",
    "\n",
    "in post.knowledge_base endpoint. when new item is added. we process that item through the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_user = \"user_2lKpUPRJD4g5IErIdhbO7rBMn3K\"\n",
    "items = supabase.table('knowledge_base').select('*').eq('user_id', current_user).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "data = items.data[1]['content']\n",
    "doc = nlp(data)\n",
    "cleaned_text = ' '.join([token.text for token in doc if not token.is_space and not token.is_punct])\n",
    "\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    encoder = encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def sliding_window_chunking(text, max_window_size=600, overlap=200):\n",
    "    encoder = encoding_for_model(\"gpt-4o\")  # Use the same model as in count_tokens\n",
    "    tokens = encoder.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_window_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk = encoder.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "        start += max_window_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def insert_chunk(parent_id, content, chunk_index, embedding):\n",
    "    print(\"func insert_chunk...\")\n",
    "    supabase.table('chunks').insert({\n",
    "        'parent_id': parent_id,\n",
    "        'content': content,\n",
    "        'chunk_index': chunk_index,\n",
    "        'embedding': embedding\n",
    "    }).execute()\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def process_item(item_id, content):\n",
    "    print(\"func process_item...\")\n",
    "    chunks = sliding_window_chunking(content) \n",
    "    for index, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        print(\"index\", index)\n",
    "        print(\"chunk\", chunk)\n",
    "        print(\"embedding\", embedding)\n",
    "        insert_chunk(item_id, chunk, index, embedding)\n",
    "#process_item(item_id=items.data[1]['id'], content=cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerank RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query, table_name, match_threshold=0.2, match_count=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    response = supabase.rpc(\n",
    "        'match_documents',\n",
    "        {\n",
    "            'query_embedding': query_embedding,\n",
    "            'match_threshold': match_threshold,\n",
    "            'match_count': match_count,\n",
    "            'table_name': table_name\n",
    "        }\n",
    "    ).execute()\n",
    "    return response.data\n",
    "\n",
    "\n",
    "def rerank_documents(user_query, top_n, docs):\n",
    "    url = 'https://api.jina.ai/v1/rerank'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer jina_b716ce28cd1b49bc920e57a5bfb6de061z36vM3vogg6y-_2d5qcoXHe_rdo'\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"jina-reranker-v2-base-multilingual\",\n",
    "        \"query\": user_query,\n",
    "        \"top_n\": top_n,\n",
    "        \"documents\": docs\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    reranked_docs = response.json()['results']\n",
    "    reranked_docs = [i['document']['text'] for i in reranked_docs]\n",
    "    return reranked_docs\n",
    "\n",
    "\n",
    "def rag_response(user_query):\n",
    "    table_name = \"chunks\"\n",
    "    results = similarity_search(user_query,table_name)\n",
    "    docs = [result['content'] for result in results]\n",
    "    reranked_docs = rerank_documents(user_query, 3, docs)\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant designed to search the company knowledge base, and find relevant information to answer questions from users.\n",
    "\n",
    "Be conversational and friendly, and at times informal, while maintaining a dignified professional persona.\n",
    "\n",
    "Where a question from the user appears to be best answered by information from the knowledge base, you will use the <context> to augment your response to the user.\n",
    "\n",
    "Where your responses involved listing, or providing of information. Format them in markdown to allow for pretty displaying to the user to enable intuitive and quick understanding of the information you have kindly provided.\n",
    "\n",
    "\"\"\"\n",
    "conversation_history = {\n",
    "    \"user_history\": [],\n",
    "    \"assistant_history\": [],\n",
    "    \"function_history\": []\n",
    "}\n",
    "\n",
    "def llm_response(system_prompt, user_prompt, conversation_history):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # full_response = \"\"\n",
    "    # tool_calls = []\n",
    "\n",
    "    # for chunk in response:\n",
    "    #     delta = chunk.choices[0].delta\n",
    "    #     if delta.content:\n",
    "    #         yield delta.content\n",
    "    #         full_response += delta.content\n",
    "    #     if delta.tool_calls:\n",
    "    #         tool_calls.extend(delta.tool_calls)\n",
    "\n",
    "    # conversation_history[\"user_history\"].append({\"role\": 'user', \"content\": user_prompt})\n",
    "    # conversation_history[\"assistant_history\"].append({\"role\": 'assistant', \"content\": response})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_query = \"tell me a little about flowon\"\n",
    "retrieved_docs = rag_response(user_query)\n",
    "user_prompt = f\"\"\"{user_query}\n",
    "retrieved docs {retrieved_docs} \"\"\"\n",
    "\n",
    "response = llm_response(system_prompt, user_prompt, conversation_history)\n",
    "\n",
    "# response_received = False\n",
    "# for response_chunk in llm_response(system_prompt, user_prompt, conversation_history):\n",
    "#     response_received = True\n",
    "#     print(response_chunk, end='', flush=True)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
