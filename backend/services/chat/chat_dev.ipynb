{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from termcolor import colored\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import requests\n",
    "from supabase import create_client, Client\n",
    "from openai import OpenAI\n",
    "from tiktoken import encoding_for_model\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_SERVICE_ROLE_KEY = os.getenv(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "SUPABASE_ANON_KEY = os.getenv(\"SUPABASE_ANON_KEY\")\n",
    "\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)\n",
    "\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking & embedding kb\n",
    "\n",
    "in post.knowledge_base endpoint. when new item is added. we process that item through the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_user = \"user_2lKpUPRJD4g5IErIdhbO7rBMn3K\"\n",
    "items = supabase.table('knowledge_base').select('*').eq('user_id', current_user).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "data = items.data[1]['content']\n",
    "doc = nlp(data)\n",
    "cleaned_text = ' '.join([token.text for token in doc if not token.is_space and not token.is_punct])\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    encoder = encoding_for_model(model)\n",
    "    tokens = encoder.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def sliding_window_chunking(text, max_window_size=600, overlap=200):\n",
    "    encoder = encoding_for_model(\"gpt-4o\")  # Use the same model as in count_tokens\n",
    "    tokens = encoder.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_window_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk = encoder.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "        start += max_window_size - overlap\n",
    "    return chunks\n",
    "\n",
    "def insert_chunk(parent_id, content, chunk_index, embedding):\n",
    "    print(\"func insert_chunk...\")\n",
    "    supabase.table('chunks').insert({\n",
    "        'parent_id': parent_id,\n",
    "        'content': content,\n",
    "        'chunk_index': chunk_index,\n",
    "        'embedding': embedding\n",
    "    }).execute()\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def process_item(item_id, content):\n",
    "    print(\"func process_item...\")\n",
    "    chunks = sliding_window_chunking(content) \n",
    "    for index, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        print(\"index\", index)\n",
    "        print(\"chunk\", chunk)\n",
    "        print(\"embedding\", embedding)\n",
    "        insert_chunk(item_id, chunk, index, embedding)\n",
    "#process_item(item_id=items.data[1]['id'], content=cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query, table_name, match_threshold=0.2, match_count=10):\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    response = supabase.rpc(\n",
    "        'match_documents',\n",
    "        {\n",
    "            'query_embedding': query_embedding,\n",
    "            'match_threshold': match_threshold,\n",
    "            'match_count': match_count,\n",
    "            'table_name': table_name\n",
    "        }\n",
    "    ).execute()\n",
    "    return response.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerank RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rerank_documents(user_query, top_n, docs):\n",
    "    url = 'https://api.jina.ai/v1/rerank'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': 'Bearer jina_b716ce28cd1b49bc920e57a5bfb6de061z36vM3vogg6y-_2d5qcoXHe_rdo'\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"jina-reranker-v2-base-multilingual\",\n",
    "        \"query\": user_query,\n",
    "        \"top_n\": top_n,\n",
    "        \"documents\": docs\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    reranked_docs = response.json()['results']\n",
    "    reranked_docs = [i['document']['text'] for i in reranked_docs]\n",
    "    return reranked_docs\n",
    "\n",
    "def rag_response(user_query):\n",
    "    table_name = \"chunks\"\n",
    "    results = similarity_search(user_query,table_name)\n",
    "    docs = [result['content'] for result in results]\n",
    "    reranked_docs = rerank_documents(user_query, 3, docs)\n",
    "\n",
    "    \n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant designed to search the company knowledge base, and find relevant information to answer questions from users.\n",
    "Where a question from the user appears to be specific to, you will use the <context> to augment your response to the user.\n",
    "\"\"\"\n",
    "\n",
    "conversation_history = {\n",
    "    \"user_history\": [],\n",
    "    \"assistant_history\": [],\n",
    "    \"function_history\": []\n",
    "}\n",
    "\n",
    "def llm_response(system_prompt, user_prompt, conversation_history):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    tool_calls = []\n",
    "\n",
    "    for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.content:\n",
    "            yield delta.content\n",
    "            full_response += delta.content\n",
    "        if delta.tool_calls:\n",
    "            tool_calls.extend(delta.tool_calls)\n",
    "\n",
    "    conversation_history[\"user_history\"].append({\"role\": 'user', \"content\": user_prompt})\n",
    "    conversation_history[\"assistant_history\"].append({\"role\": 'assistant', \"content\": full_response})\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_query = \"tell me a little about flowon\"\n",
    "retrieved_docs = rag_response(user_query)\n",
    "\n",
    "user_prompt = f\"\"\"{user_query}\n",
    "retrieved docs {retrieved_docs} \"\"\"\n",
    "\n",
    "response_received = False\n",
    "for response_chunk in llm_response(system_prompt, user_prompt, conversation_history):\n",
    "    response_received = True\n",
    "    print(response_chunk, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
